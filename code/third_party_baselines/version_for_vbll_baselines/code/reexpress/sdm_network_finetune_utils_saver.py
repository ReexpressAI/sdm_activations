# Copyright Reexpress AI, Inc. All rights reserved.
"""
Callback that handles coordination of hard negatives generated by each device during training.
"""

import logging
import sdm_network_constants

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)

import json
import torch.distributed as dist
from transformers import TrainerCallback
import os


class GenerationSaverCallback(TrainerCallback):
    def __init__(self, save_dir):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        self.trainer = None
        self.remove_intermediate_files = False

    def load_and_return_existing_consolidated_generations(self):
        """Load consolidated generations from previous runs"""
        current_global_all_generations_at_epoch_start = {}
        consolidated_file = os.path.join(self.trainer.generation_save_dir,
                                         sdm_network_constants.FILENAME_TRAIN_TIME_HARD_NEGATIVES_GENERATIONS_FILE)
        if os.path.exists(consolidated_file):
            with open(consolidated_file, 'r') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        doc_id = data.get(sdm_network_constants.REEXPRESS_ID_KEY)
                        if doc_id:
                            current_global_all_generations_at_epoch_start[doc_id] = data
            logger.info(f"Loaded {len(current_global_all_generations_at_epoch_start)} existing generations (training)")
        return current_global_all_generations_at_epoch_start

    def on_epoch_begin(self, args, state, control, **kwargs):
        trainer = self.trainer  # kwargs.get('trainer')
        print(f"At on_epoch_begin for rank {args.local_rank}. trainer has the method: "
              f"{trainer and hasattr(trainer, 'save_generations_rank_aware')}\n")
        if trainer:
            print(f"Rank {args.local_rank}: len(trainer.all_generations): {len(trainer.all_generations)}\n")
            # Clear existing and update:
            trainer.all_generations = self.load_and_return_existing_consolidated_generations()
            print(f"Rank {args.local_rank}: len(trainer.all_generations) after global synchronization: "
                  f"{len(trainer.all_generations)}\n")
            if (trainer.use_cross_entropy or trainer.use_dpo) and trainer.reset_generations_every_round:
                # Note: this occurs in VerificationLayerCallback.on_epoch_begin for the SDM loss case
                print(f"Resetting generations in preparation for the next epoch")
                trainer.all_generations = {}
                if dist.is_initialized():
                    dist.barrier()

        else:
            logger.warning("Trainer not found in callback, cannot synchronize generations.")

    def on_epoch_end(self, args, state, control, **kwargs):
        trainer = self.trainer  # kwargs.get('trainer')
        """Save and consolidate generations at epoch end"""
        print(f"At on_epoch_end for rank {args.local_rank}. trainer has the method: "
              f"{trainer and hasattr(trainer, 'save_generations_rank_aware')}\n")
        if trainer:
            print(f"len(trainer.all_generations): {len(trainer.all_generations)}\n")
        else:
            logger.warning("Trainer not found in callback, cannot save generations.")
        if trainer and hasattr(trainer, 'save_generations_rank_aware'):
            # Each rank saves its own file
            trainer.save_generations_rank_aware(state.epoch)

            # Synchronize
            if dist.is_initialized():
                dist.barrier()

            # Only rank 0 consolidates
            if args.local_rank in [-1, 0]:
                self.consolidate_jsonl_files(epoch=state.epoch, is_training=True)

        # Final barrier outside the if-block to ensure ALL ranks wait
        # for consolidation to complete before proceeding
        if dist.is_initialized():
            dist.barrier()

    def on_evaluate(self, args, state, control, **kwargs):
        """This is called AFTER evaluation ends.
        We use this to consolidate hard negatives from the validation set. The equivalent of the on_epoch_begin
        to load the existing generations occurs in the override of evaluate of Trainer."""
        # Similar logic to on_epoch_end, but for the validation set generations
        trainer = self.trainer
        # Each rank saves its own file
        trainer.save_generations_rank_aware_validation_at_step(global_step=state.global_step)
        # Synchronize
        if dist.is_initialized():
            dist.barrier()

        # Only rank 0 consolidates
        if args.local_rank in [-1, 0]:
            self.consolidate_jsonl_files(is_training=False, global_step=state.global_step)

        # Final barrier outside the if-block to ensure ALL ranks wait
        # for consolidation to complete before proceeding
        if dist.is_initialized():
            dist.barrier()

    def consolidate_jsonl_files(self, epoch=None, is_training=True, global_step=None):
        """Merge JSONL files from all ranks"""
        world_size = dist.get_world_size() if dist.is_initialized() else 1
        all_generations = {}

        # Read all rank files
        for rank in range(world_size):
            if is_training:
                rank_file = os.path.join(
                    self.save_dir,
                    f"generations_rank_{rank}_epoch_{epoch}.jsonl"
                )
            else:
                rank_file = os.path.join(
                    self.save_dir,
                    f"generations_validation_rank_{rank}_global_step_{global_step}.jsonl"
                )

            if os.path.exists(rank_file):
                with open(rank_file, 'r') as f:
                    for line in f:
                        if line.strip():
                            data = json.loads(line)
                            doc_id = data.get(sdm_network_constants.REEXPRESS_ID_KEY)
                            if doc_id:
                                # Need to merge to avoid overwriting:
                                generations_in_rank = data.get(sdm_network_constants.HARD_NEGATIVES_KEY, [])
                                most_recent_epoch_in_rank = \
                                    data.get(sdm_network_constants.SDM_NETWORK_FINETUNING_EPOCH_KEY, 0.0)
                                if len(generations_in_rank) > 0:
                                    if doc_id in all_generations:
                                        running_cumulative_generations = \
                                            all_generations[doc_id].get(sdm_network_constants.HARD_NEGATIVES_KEY, [])
                                        running_cumulative_most_recent_epoch = \
                                            all_generations[doc_id].get(
                                                sdm_network_constants.SDM_NETWORK_FINETUNING_EPOCH_KEY, 0.0)
                                        # merge:
                                        all_generations[doc_id][sdm_network_constants.HARD_NEGATIVES_KEY] = \
                                            list(set(generations_in_rank + running_cumulative_generations))
                                        all_generations[doc_id][sdm_network_constants.SDM_NETWORK_FINETUNING_EPOCH_KEY] = \
                                            max([most_recent_epoch_in_rank, running_cumulative_most_recent_epoch])
                                    else:  # new entry
                                        all_generations[doc_id] = {
                                            sdm_network_constants.REEXPRESS_ID_KEY: doc_id,
                                            sdm_network_constants.HARD_NEGATIVES_KEY: list(set(generations_in_rank)),
                                            sdm_network_constants.SDM_NETWORK_FINETUNING_EPOCH_KEY: most_recent_epoch_in_rank
                                        }

        # Save consolidated file
        if is_training:
            consolidated_file = os.path.join(self.save_dir,
                                             sdm_network_constants.FILENAME_TRAIN_TIME_HARD_NEGATIVES_GENERATIONS_FILE)
        else:
            consolidated_file = \
                os.path.join(self.save_dir,
                             sdm_network_constants.FILENAME_TRAIN_TIME_HARD_NEGATIVES_GENERATIONS_VALIDATION_FILE)
        with open(consolidated_file, 'w') as f:
            for doc_id, data in all_generations.items():
                f.write(json.dumps(data) + '\n')

        # logger.info(f"Consolidated {len(all_generations)} unique document ids (training: {is_training})")
        print(f"Consolidated {len(all_generations)} unique document ids (training: {is_training})")

        if self.remove_intermediate_files:
            # Optionally remove intermediate files
            if is_training:
                for rank in range(world_size):
                    rank_file = os.path.join(
                        self.save_dir,
                        f"generations_rank_{rank}_epoch_{epoch}.jsonl"
                    )
                    if os.path.exists(rank_file):
                        os.remove(rank_file)
            else:
                for rank in range(world_size):
                    rank_file = os.path.join(
                        self.save_dir,
                        f"generations_validation_rank_{rank}_global_step_{global_step}.jsonl"
                    )
                    if os.path.exists(rank_file):
                        os.remove(rank_file)
